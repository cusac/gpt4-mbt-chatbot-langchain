XXXTom CampbellXXX

Remember  what we were talking about here was that we had decided that about eight particles a second. Now we just made these numbers up. But a lot of people get a sense of concreteness if you give them a number. And they kind of get the idea that numbers. So I'm doing numbers here just to give you a sense of concreteness to the concept. Otherwise it gets for non mathy people, if it's not concrete, it's kind of hard to get hold of. It slips away. Okay, so that's where we were and we're going to go on to the next one. All right, so here's what all of this has led up to. Now, you know how these double slit experiments work. How it is they fire a particle and has an equal probability of going in one slit or the other. You see they're really firing lots of particles, and some of them land in between and whatever. But on an average, we're going to get about eight particles go through the slits, and on the average, about four of them will go through the right slit and four of them will go through the left slit. So we're talking probability and statistics here. It's not that they fire one particle and it has an equal probability of going through both slits because they just have poor aim. That's not the way it's done. Okay, now what we've done then is that we've created this standard double slit experiment with which way data notice that these detectors are turned on, the recorders are turned on. So you're going to get this two bar result. Here's how the larger conscious system does this. First, it takes a random draw, the particle generator, to see how long it'll be before the next particle comes out. So it knows when the next thing's coming. And it did that the last time before that particle went out. So it knows when this particle is coming out. So that's how it knows what's coming out. And it's from this distribution. So it's not always the same, but on an average, it's going to be about eight particles per second. The second thing it does, if it's an electron or some massy particle and not light, it has to decide how fast is it coming out? It's the same way. It could be real fast, it could be real slow. Or it's mostly right there in the middle that's they have a little more control over. And that's another random draw. So that's the second random draw. Third random draw is up here where they say, okay, what slits it going to go through? Because we're measuring slits, we need to know. Takes a random draw from a binary distribution, picks a slit one or a two random draw. When it does that, if, say, it draws a slit one, then it knows that within a short time, it has to write something on R one. And with a short bit of time, it's going to have to put something on this screen. So it comes back here to these two single value probability distributions. And it knows it has to pick from this one. So it goes into this single value distribution, one for slit one, and it says, all right, I'm going to pull out x one. Well, it knew that just because this said x one. So that's what's going on. So we have the one, the two, the three. Then it comes here, there's a four, and then it needs a Y coordinate. Where is it going to be in Y on this thing? And it comes here to a random draw where it just picks a random number, and that's where it puts it in Y. So we've reduced this whole experiment to five draws for probability distributions. That's it. So now you're the conscious system. Somebody's put this experiment together. A particle spits out. You don't have to generate particles. You don't have to worry about how hot's the filament or this and the light and the lasers and the atoms. You're not modeling all that stuff. All you're doing is taking five draws, 12345 experiments done for that particle. And you do that for every particle. Well, a draw from a probability distribution, like we say, that's a nanosecond worth of time. That's not calculating and modeling all the stuff that has to go into this experiment. So you see the larger conscious system, five nanoseconds, it's got everything calculated. It didn't break a sweat. It's easy. It's just pulling numbers out of distributions. That's what we mean by a probabilistic model. So it's just that simple. So there really is no particle generator in that sense. It's not actually being modeled. There is no slits, the screen, all of that. These major components, they're what we call eye candy. They're really non functional elements of the experiment. As far as the logic kind of system is concerned, it has the logic of this setup. When the scientists built this, there's a logic that went to it. It has to follow that logic. Then all it has to do is give the particle generator the slits. All that stuff is just a candy. It's there because the scientists see it, that it's there. So they get a data stream that it's there. But as far as the calculations, it's all irrelevant. Really doesn't make any difference. Five random draws, and these last two picking from a single value. You know the value. As soon as you pick this, you know that's one. So it's not really much of a thing to do. And as far as this one, it's just a random number. So there's not a lot of thought into that, too. These two require the system to create probability distributions ahead of time based on the logic that these experimenters put together. And how can it do that? Well, all this stuff, it's a virtual particle generator. It's virtual slits in a virtual room in a virtual university. You see, all of it is just calculated already. So it's real simple for it to take that model that it has of this virtual experiment, run a few hundred thousand samples, generate these distributions, and now all it has to do is and it can do that while everybody else is sleeping. It can do that very easily. It's got a lot of cycles to have. Even between our Delta T's, the time where there's no time for us, we have a delta T, then we get the next delta T. We jump ahead with these little units of time just between those jumps. The larger system has billions of cycles to spend on doing things. So it's not hard for it to do what it has to do as overhead to maintain the simulation. That's an easy thing for it to do. So you see that. So you have various questions. The first question is, when does the next particle arrive? That comes from here and here, it arrives at these slits. It arrives at the screen. You can calculate that from knowing when the particle is coming and how fast is it moving. That's all you need to know. So all the rest of the stuff in the experiment is eye candy. You can think of an analog of this when you talk about the logic of the building. Let's say we're in the world of warcraft again, and let's say the elf can build a wall to keep the barbarians out. And there's a rule set about how this wall is built. But as far as the player goes, the player goes over and it gets a block, and it sets a block down and sets another one, and they connect and it sets another one. It keeps doing that, and it puts blocks on top, and it builds a wall, okay? And once it builds its wall high enough and wide enough, the barbarians can't get in. That's the idea. It's the way the game works. Well, what really keeps the barbarian out? The rule set, right? It's the rule set that keeps the barbarian out, not the wall. The wall is just eye candy. It represents the logic of the rule set. The logic of the rule set is you build this wall, the barbarians can't get over it. So that's how avatars in a virtual reality build things. They abide by the rule set. Well, that's how this experiment got built. Bunch of physicists in a lab bought the laser. They bought all the rest of the stuff. And all of that stuff represents logic. It does certain things. It has certain capabilities, certain ranges, and they turn buttons and throw switches and tune knobs. And when they do that, it changes the logic of the system. And when they're done and they say, all right, let's push the run button and do the experiment, that logic then, however it is just then will be run, and the experiment is five random draws, two of which are hardly worth mentioning. All right? So that's how that works. So it's the logic in the rule set that keeps the barbarians out. It's the logic here in these five random draws that actually is doing all the work. The rest of it's just there. Okay, now we're going to do the other one, which is the one where you don't have any which way data. So now we have these detectors are turned off, there is no data. And of course, when there is no which way data, we're going to get this diffraction pattern. Now, here what happens. It's fewer. First we have when's it coming, how fast is it going, which tells us the times. It tells me the time when I have to write something in R One, and the time that I have to write something in R Three. If I know those two things, then I know exactly when I have to make those writes. Because the only thing that's coming out to the consciousness player is R One and R Two and R three. That's it. That's the only interface to the humans. And this whole thing is only being done because the humans asked the question. They're making the measurement. All right? So we have those two one and two number three. We don't have any binary thing. It doesn't matter about these slits, you see, because we're not measuring it. So if it's not going out to the people, it doesn't happen. So we don't worry about slits. What slits? Who needs slits? Right? We're going to go right to the screen. Slits are irrelevant. Now, see, we're not having a probability distribution fly through the two slits and then combine in some way to make this pattern. The slits are irrelevant. They don't really play in this experiment because there's no information that has to go out to an observer. Therefore, there's nothing to do. The system only has to do those things. It creates information out to the observer. So the third thing it does then, after it does one and two, is it comes here and says, oh, at a certain time I have to put a dot on this screen. And I know because of the logic of the experiment, that I have to draw that from this diffraction pattern distribution. And how did I get this diffraction pattern distribution? Well, between those Delta T's, I ran a whole lot of samples of this logic to see exactly what it would do. And this particular logic with those particular slits, with that particular generator and all the rest of the logic going into this produced this particular diffraction pattern probability distribution. So the third thing I do is I go in and take a random draw out of this. And let's say I land on this little peak of this little point over here. Well, I stick that somewhere in random in this y, in this last one over here. If I get it in this big peak, then I'd put it someplace in here. So I stick the point on the screen. So you see, it's not a matter of probability moving through slits and that sort of thing. That's just metaphors because people who are materialists can't talk about it any other way and whether there's something moving through the slits to do something. But the slits aren't relevant in this experiment because none of the data from the slits is going anywhere. So they're not even modeled. They're eye candy. They're irrelevant. It's just a selection from this distribution. So see how that works? That's a lot different than what the physicist would tell you because they want something to actually combine probabilities through some kind of path to end up with that diffraction pattern. Because they need material process. They need a trail of causality because they don't like action at a distance. It's unnerving. That stuff just happens over there and you don't have any cause for it other than the rule set. So that's what's going on. We've done the same thing here. So you see here's the random in the Y here's? That distribution. Here's the kind of distribution that this creates, this probability distribution. This is one that was actually taken with samples, right? You just run the program a lot of time, create this out of samples. That's your probability distribution. The only real measurement there, of course, was at R three, right? Because R One and Two didn't take any data. So that was the only measurement. All right, we're going to introduce the experiments now that we that's all our background. All right, we're ready to get serious now. I make a couple of points here. That what we're doing. I think I've covered them mostly. Feynman noted that the Double Slute experiment held the basic mystery, the basic solution to quantum mechanics. He's correct. It does. If you understand that, you understand the whole thing. Have to realize that our virtual reality is generated by an intelligent simulation. Okay? It's not just a deterministic machine. That's a tough one for scientists to get over. Ever since Isaac Newton talked about the clockwork universe we've had this fantasy that our reality is plug and chug. Physicists will tell you that if they knew the location and the velocity and the charge and the spin whatever of every particle in the universe, they could tell you from then on, everything that happened there is no free will. It's deterministic. All you have to know is the beginning stuff, get the physics right and from then on, you calculate everything else. That's what they believe. Because you have to believe that if you're a materialist, there's no other way for materialism to work. If you don't believe that, so they're forced to also become determinists which is an uncomfortable place to be because then they have to realize that they don't have any free will. They're not really doing anything. They're puppets in a movie going through the motions, and I don't think they like that either, but that's the conclusion that they come to. Okay, what you're going to see is five different double slit experiments with a dozen or more of sub experiments. Some of those subexperiments are just introductory. We have to do this step to get to that step. Some of them are conditional, which means if this step works this way, then here's where we want to do these experiments last. If it works a different way, we're going to do these other ones after that. What I'm doing is taking quantum mechanics as it exists today and redescribing it in terms of virtual reality. That's the point. That's why I said there is nothing going through slits. There's not even any slits. That's eye candy. It's just drawing from distributions. That's the whole thing. So what we have then is we're going to take quantum mechanics, and we're going to reduce it to just logical understanding. If you understand the logic, then you can tell what's going to happen to the result. Well, that's a real great leap forward for quantum mechanics, because now with quantum mechanics, you take the logic of the setup, and then you do calculations for about three weeks, tie up a computer for a bunch of hours, and then you can say what the result is going to be. It's all calculational. You can't tell what happens till you do the math, and talking about it before you do the math is just a waste of time. There is no logical way to tell what's going to happen because it's not logical. It's weird science. You don't know till you crank the equations. This way. You can look at any of these experiments, understand the logic, and say, here's what's going to happen, and here's why. So now it's become a rational science rather than weird science. It's just like any other science. It's not that hard to understand. All right, some basic characteristics. These are all the things that we've talked about up to this point. I don't need to go through them again. I wanted to collect them all in one slide so the people going through this later would find all of this together. There's everything there we've already discussed, and none of it should be a surprise to you. So when you get back, if you want to look at this later when it's on YouTube or if you want to get the DVD or something, then you can go back and read all this, and it just puts the stuff that we've said into a nice one place. The two that we maybe haven't talked about are down there on the bottom. The random draws will be finalized according to current objective conditions. When no future changes in the results of that specific measurement are possible, that means that all potential changes or options have been taken. So the logic becomes fixed. A lot of times the logic of these experiments can be changed. And that's what we do. See, what we're trying to do is trick the larger conscious system into giving up some of its secrets. That's the point. So we'll set the system working on a logic, and it'll get halfway through, and then we'll jerk the rug gut from under its feet and change the conditions of the experiment, say, now, what are you going to do? You've already committed. You've already written data on these two things that say it has to work a certain way. And now we've changed that experiment. Now, right at the end, before you got to the end, you see, that's how we trick it into giving up some of its secrets. So as long as things can be changed, the system hedges its bets. It knows that it could have that rug pulled out from under it. So it plans how it's going to run this experiment based on that, and it has its own strategies. So it's this game, can we catch it and can it not let us? Kind of a game. So that's what that says. As long as there are changes possible, then what does it say? The game's not over till the fat lady sings. Right? As long as there's changes possible, then the fat lady hasn't sung yet, and hard to say how it might turn out. All right, that I know you can't read. Even I can't read that. That is meant for people who get the you know, who get the video later. What I've done now is I've taken fundamental virtual reality characteristics that we talked about, and I've written down here how to apply them to this quantum mechanics. And I've done that all on one page because I want it to be there, because the first sentence is important for the next sentence, and so on. So you need to see it all together. But I can tell you a little bit about what's on there. One of the things so you don't have to read it. I don't expect you to read that. It's just there, but I wanted it to be there and show you to encourage you to get the DVD, go to the YouTube and whatever, and you'll get all this nifty stuff extra. All right, so what's on there is a couple of things that I can tell you. One is that in virtual reality, time is fundamental and always runs forward, right? Because there's a Delta T. Every Delta T is another time. Well, it's really hard in a simulation to say, oh, I want to go back six Delta T and change something. Time's marched on. It's really hard. So you have to do things in the order that they come. Time is fundamental. It's fundamental because that's just the Delta T creates our time. Every time the simulation loops through Delta T, our time gets incremented by that little bit. So we can't just jump around in time. Time's fundamental in our reality. Now, when I say it's fundamental that Delta T is only for our reality there's some other virtual reality and they have another Delta T. They don't have anything to do with ours. It's not that time is fundamental for these various virtual realities. All the same. They all have their own clock. How much resolution do you need? That's the case. All right. Another thing that's in there is that we define what constitutes a physical fact. These definitions start to get real detailed. We define in there what a physical fact is and what about its perishability. Facts don't last forever. Facts come and facts go. Information comes and goes. You may have a blip on a screen and it's only there as long as the persistence of that screen is. And after that, it's gone. It's not there anymore. So you had information, but it's gone now. So it's very perishable. And what does perishability do to it being a fact? How long does it have to hang around before it's a fact? And how many people have to see it before it's a fact? You see it's this sort of thing. These are all important. Number of people have to see it before it's a fact. Important because the only thing that makes a fact in this physical reality is somebody got data in their data stream. If nobody got data in their data stream, there is no fact. There are no facts other than what you get in your data stream. So if the person is involved, you don't have any observer, no person, then there's no data. Nothing changes. The physical reality remains unchanged unless data stream changes something. Okay, so we also define a common data space called what we call the physical universe. And what is a common data space? Physical facts are objective. That means they're shared with all the other players pieces of information that are, for at least some period of time available to an individuated unit of consciousness within PMR. In other words, you need to have some consciousness to see the data before it becomes physical here before it becomes a virtual physical. You know what I mean when I say physical, I mean this virtual reality. Okay, we define objective versus subjective data deductive versus inductive logic. We define how common data space works in a multiplayer game, though recorded which way data is always sufficient to cause decoherence. Now, I'll go back to that word in a minute. Recording the data may not be always necessary. When is it necessary? When is it not? That leads to the possibility that if the which way data is ever an objective fact available within the shared space of PMR long enough for people to get a good look at it, and if one or more people do get a good look at it, then that instance of which way data is enough to cause decoherence. Now, what is decoherence. Decoherence really doesn't mean anything at all. It's one of those terms that because it's really embarrassing to a physicist to call that pattern a two bar pattern. That sounds like something that they might say in high school. It's a two bar pattern. So they say it's a decoherence pattern. And that comes from the optics experiments. In optics, you have to shine a coherent light source. That means all the little white layers remember those wiggles we saw that would get out of phase and in phase they all have to be going together when they get to those two slits. That's what coherent means. They're all bumping and humping and dipping at the same time. Now, if you just shine a flashlight, that doesn't produce coherent light. That light's coming out of that little heated element. There's all sorts of different ways and that won't make such a good diffraction pattern. So when the diffraction pattern changes to two bars, a $5 word for that is decoherent. What it's made is it's created decoherence in the source. Because if your light went from coherent to decoherent, it would go from a diffraction pattern to the two bars. So that's all it means. Decoherence is just two bars on the screen, one behind each slit, no longer coherent light. But as you see, it has nothing to do with coherence. It has nothing to do with that at all. That's irrelevant. But it's classier than two bars. So that's what we call it. Another thing that I use, two things that I figure out how virtual reality will implement these experiments. One of them is I expect the system to be efficient. So if there's three ways that they could do this and one of them is much more straightforward than the other, that's the one that I back. I say, well, this is the way it's going to happen because this system has been around an awful long time. It seems to be very good at doing these simulations. I doubt that it's a real Klude of a system. It's probably as fast and efficient as it can be. It's had plenty of time to evolve to that. Remember, this is an evolving system. So that's one of the ways because sometimes there's multiple ways that you can get to a point and I pick the way that is most efficient. In computer talk, that's the most parsimonious, the stingiest way to get there with the fewest number of statements and the most elegant approach, making a measurement in a PMR. We've also sort of discussed this. It pulls together what we've said about making measurements. A measurement isn't made until an IUOC individuated unit of consciousness asks for data. I'm going to make a measurement. I want data back. The data is my measurement. Larger conscious system. Or if you like, the virtual reality rendering engine. The thing that creates this reality sends me data back. I get that data. I interpret it as physical reality. I the consciousness interpret it as physical reality. That's physical reality, then physical reality is really just the interpretation, the group interpretation of all the individuated units of consciousness that are logged on playing the game. It's all interpreted in their head, just like World of Warcraft. There isn't really some field with trees and rivers. It's all just in the minds of the people getting the data. This is the same way. It's all just in the minds of the players, the players of the consciousness, not the avatars. So this works exactly the same. All right, I think we can go on. This one I put just because it's interesting. It's not yet. I'm not giving these experiment numbers yet because these aren't really my experiments. We're still kind of getting spun up. Okay, this was an experiment that's been done, and what they do is this is on, and this is on. So we have one slit that the data is being taken, and this one's off, and that one's off. And the point of this is what happens? What are you going to see? So you have data with one slit and not the other. Well, this makes a very important point that I wanted to bring up, and that is that if the logic of your experiment is deductive, deductive means there's no wiggle room. If this, then that, then that's as good as the data. So Deductive logic can fill in for the actual data. And that's what we have here. So we have a particle that comes out, and we said eight particles a second, four a second going through each slit. So four times a second, something will go through this slit, and something will be written there in R one. And then they'll go pick one of these single value distributions and stick it right here in this slit one pile, someplace with a random Y. All right. Now the pulse comes through, and there was an account here because we know there's going to be four particles a second going through the second slit. We just know that that's the logic of the setup. Right? All right. So now I'm the system one's gone through that other slit where there's no measurement. What am I going to do? Where am I going to put it? See, if I decide to put it over here, like in part of a diffraction pattern, I'm going to have a problem because somebody's going to say, well, if it's over there, it must have come through slit two. Because if it went through slit one, it'd be right here. But if I know it went through slit two, then it has to be right here, because once I know it's through slit two, it just has to hit behind slit two. So Deductive logic tells you that anything that doesn't hit here will have to come through the other slit, which means it has to hit here. So even though there isn't. Any which way data for this slit at all. You get these double bars. In other words, deductive logic fills in for the which way data on that one side. That's an interesting point, and that becomes important later. But this is an experiment that's been done many times, and that's the way that works, and that's why it works that way. Okay? See, that's how these experiments go. The way you learn things is you try to create a problem for the larger consciousness system. You try to trick them into a hard problem that they don't know how to answer. So in this case, you say, yeah, but we don't measure any particles going through slit two. So you need to make them a diffraction pattern because that's the rule. And no, it isn't because deductive logic says not. All right, so we're going to the next one. Now we finally made it to experiment one. And this is just really a cover slide for experiment one, because experiment one is all of these experiments, there's a one A, one B, one C A. See, have these single asterisks on them. I don't know if you can see that or not, but the first three all have anonymous detectors. That means detector just sends out a pulse. One pulse is like any other pulse. So the pulse that comes out of this detector D one is exactly like the pulse that comes out of that detector D two. And if all the lengths of the wires and things are all exactly the same by the time they get to this point, which way information is erased. Because when they get down here into this R one, it can't tell one pulse from the other because they both come together at that one point. So you see, this is a very simple way to erase the data. I tried to keep these experiments simple because they have a higher probability of being done if they're simple and cheap. So this is a simple eraser experiment. You have which way data here and here because it's on. But by the time you get past this point, it gets erased because gets down here, it just says, a pulse arrived. Another pulse arrived. It doesn't say pulse from one or two. Okay? So this is just a regular eraser experiment because it's eraser experiment. What we expect to find is when it gets around to what's on the screen, it's going to pull it from this distribution. Now, if we make these detectors to where they're unique. So now this detector says, got a pulse coming from one. And that's the message that gets recorded here at one pulse just came in from one. Now we've got which way data. So if they are unique in some way, then we collect the which way data. And then it has to draw from this distribution. So it depends on whether we have these unique or whether we have these anonymous. So we can turn this back and forth. Just by switching these two, we can turn it back and forth between anonymous and unique which way data, not which way data. Okay, so that's what this is about. Again, we have one, two, three draws. And then the fourth one, of course, is it'll come from one of these, depending on which one of these you pick. So there's really just those draws and the Y being random. We just won't even mention that anymore because we all know that's really not too important. Okay, so that's what's going on. And these first three experiments we're going to get to here in one are all going to be with this anonymous. The second three are going to be with that has information which way information. Now, so here's one A, they're anonymous, just what you'd expect. Everything is as you would expect it. Yes, the data gets recorded. Yes, it gets erased. Since it gets erased, we get pulled out of this diffraction pattern and we have the one, the two and the three random draws to get there. Okay, so nothing surprising there. Now, next, something's different here. What's this? We've just extended those wires, or whatever they are, call them wires if we like. That's easy enough there's. The detectors make electrical pulses. We extend those wires and we can make them as long as we want. And how far we want to make them is so that this distance R one from the slits here all the way out to R one, is much longer than this distance DS between the slits and the screen. What's going to happen now is that the which way data is going to be in this one is going to be in this lane. Slit two is going to be in that lane, and they're going to be chugging along in their lane, existing there long after this has already been done. So we're going to have which way data that exists in these two, while the system is forced to put a dot on this screen. And will the system decide that that which way data is available because it's traveling down these two separate wires? Or will it decide it's not available because when it gets here, it's going to be erased? Well, there's no measurements in here. There isn't anything that comes out to our reality until we get to this measurement. So we don't care whether there's which way data here, it's just not available and it won't be available. Now, if we change the experiment and put some kind of little measurement in here to put a signal on them to let us know what Slit was, then that's a different experiment. You'd get a different answer. But as this goes, no, we don't expect any of that to happen. They're going to get erased here. So just because there's which way data in the system when this screen dot has to be put on makes no difference whatsoever. Now also, I've got these things stacked and I just wanted to mention that it doesn't matter really how much data you collect here. You're only going to collect a time because these are anonymous. You're not going to get any which slit data. You're not going to get an X here. You got to get an X. You could get X only. That's fine. That tells you where it is. On this X axis, which bar this is in, you could get an X and a Y, which lets us make the bar look tall. Or you get an X, Y and a T and any of those will do. So if you limited it just to an X, you'd still experiment would come out just the same. You don't have to have those others. Okay, so that's how that one works. Now this is a delayed erasure experiment, probably the simplest delayed erasure experiment you've ever seen. Usually delayed erasure experiments take you an hour to figure out what the setup is like. They're very complicated, but this one's very simple. So that's all there is to that. We get a diffraction pattern because the logic says it's going to be erased, therefore, we get the diffraction pattern next. Now we're going to get tricky. This is the first time where we're really going to put the larger conscious system between a rock and a hard place. We're going to put a which way device that randomly injects which way path uniqueness just before the two paths converge. In other words, just before these paths get to where they go into this common point and the which way date is lost, we're going to stick a little thing in there that will stick a, this is path one, this is path two. We're going to add the which way data in it, but we're not going to do it until after the system's already written the data here. So when the thing starts out and the particle comes through here very quickly compared to here, because this path is much shorter than the path out to here. Remember, this is a long path. Now, even though I ran out of slide room and it doesn't look so long, it's really a long path and it's going to have to write this data to the screen while these things are turned off. Has every expectation of this being a diffraction pattern. That's what we just saw. Okay? But now right in the middle we're going to switch that and add which weight data to it. Okay, well, what's the thing going to do? Here's the way it's going to approach this problem. We've got our one and our two for our random draws. The third random draw is going to be over here. We're going to have a random draw of this binary distribution. I call that binary distribution one screen one. And when it does that, it's not going to be picking which slit. It's going to be picking what do I want to put on this screen? Do I put a diffraction pattern distribution or a single value? Do I put this one or do I put this one there? Because it knows that somebody's going to pull the rug out from under diner or they could. So it needs to hedge its bets. So it's going to, right here, ahead of time, say which one it's going to pick. So let's say it picks a diffraction pattern, says, all right, I'm going to put it in a diffraction pattern. I go down here, I make my random draw, and I get this little point which will put it somewhere in that line. So it does that. Now what happens next? What happens next? Because I picked that diffraction pattern, if the result from BD one is a diffraction pattern, it's going to turn this little which way injector off, and then it's done. So it's going to work just like so in other words, we've got this out of sequence right, for the system. That's why we're putting it between a rock and a hard place. So it's actually going to make this be an off. Well, how can it make that be an off? The way we'll run this experiment is whether it puts that which way data in there or not, it's going to be random. We're going to have a random binary thing that'll decide to turn it on or off. And it won't make that decision until after this screen has already been written on. So after the screen has been written on, then it's going to decide whether that's going to be on or off with a random generator. The way that's going to work is that this is going to be a radioactive source. This is two hemispheres and they're going to close in basically around this radioactive source. Each one of these hemispheres is like a Geiger counter when this radioactive thing sends off a particle and if it hits this side, okay, that's a one, say. And if it hits that side, it's a zero. This is the binary. Get a one or a zero off or on in this case with the switch. So if the particle comes out, hits this side, say it's off, and hits this side, say it's on. Now, there's one thing you know about random radiation. I mean one thing you know about radiation, the way atoms decay is that it's random. There's a random physical event that this will decay in any direction, but we've got it covered here with these two hemispheres, but it only can hit in one of them. But it's random and it's a massive particle. We know it's random and it's just going to decay. And that's going to be our random number generator. And when it happens to hit, when the particle comes out here in any which way happens to hit this one, it'll turn that switch, whatever I said off. If it hits this one, it'll turn that switch on. And we're not going to activate this until after the particles hit the screen and the results already been written. So what I've said up here that's going to happen is that the system will pick this binary of which one it wants to display because it has to do this first. It's not impossible to go back in time and do things if you're a computer simulation, but it's hard. You can mess up a lot of stuff if you go back and fiddle with stuff that's already done and move forward in time, because everything moves forward and time changes everything else. So it's hard to do. So then it's going to have to make that random draw of that radioactive element suit what it did here earlier. This is a major miracle that it's going to be able to do that. Now, how will you know that it's done that? Because if I say that, you can say, yeah, okay, we can pretend that's it. But is that really what's going on? Unfortunately, there's a way to verify this. So you see, that's the issue. Now let's look at the other one. If it picked that it was a single value, that that's what it was going to write here. It writes that. It writes one of these. You just get two stripes. And if the result from DB one is a single value, then it's going to switch that which way on. That's going to make a draw from another binary distribution for the which way data, which will actually now, of course, the slit's already been gone through and all the rest of it, it'll say what slit it went through, write the data R one at the appropriate time. So now we're determining the slit that it went through after it's hit the screen and after this has been switched. So those are the two paths that it can take. All right, and the next slide tells you how we're going to trap the consciousness system and see if this is really what they're doing. See, this is kind of out there, right, as far as how this goes. It's all out of sequence. It's writing this data. The result data before the experiment has been done. There's a random, what they call an event based random generator. And it'll work the same if it were computer based random generator. But this is a real random number from an event based random generator. All right, now this is a whole lot of information about what's going on and why and how and all that kind of stuff. And you'll get to read that if you get the DVD, right? This sounds like a sales pitch, and if you buy our DVD, you get to read it's. Not like that. It's just that it'd take too long to go over all of that and it'd be very tedious and everybody would lose interest and fall asleep. We don't want that. So what we've added here, this is what we just looked at. What's been added here is a microprocessor. Now what this microprocessor is going to do is first of all we're going to adjust the parameters in this experiment such that the double bars this is turned on, you'll get the double bars so that the double bars conflict as little as possible with the diffraction pattern. And you can do that by distance between the slits, the wavelength of the energy. You saw how the wavelengths did that. Well, you can change that geometry such that you can move those slits around and we can make the diffraction pattern spread out by making a little more distance between the slits. So we can do that, we can spread it out enough that we can move it and locate it such that you can see the I don't know if you can see that or not, but there's little x one and x two little lines that go up. They go right up between these two lines and right up between those two lines. And it may be more likely they'd go up on either side of the center peak. In other words, be going right in here in this null. That would be one of the bar slits and that would be another one. So we want to adjust our parameters to minimize the overlap between these two patterns. Okay, next thing we're going to do is we're going to make two really long data runs and we're going to run it either with the random thing going or just run all with the detectors on and all with the detectors off. But we're going to get lots and lots of data from this and the data we're going to get from it, we will know exactly what point correlates with what data here in R one. We're going to have time so we'll know exactly the time that something's going to hit here. We'll record that time x, Y and T. We're going to know exactly what time something gets here because we can do that. So we'll time it. So we'll look at this and say, oh, this came in at exactly that time and this came in at exactly a second later because it's a second longer path so we can correlate those two. So every piece of data that's in R One I can match with a point on this screen and I'm going to do that when I run this. I can run thousands and thousands and thousands of points in both configurations. Then I'm going to put those points together. I may even have done a slide of this. Yeah, I can put those points together. Let me jump here. And that's what it's going to look like. Those two purple things or blue things in the middle, those are the fraction pattern. I mean, that's the single pattern. Those are the slits here and here. But they don't overlap much. Now in the real world it isn't as neat. There is going to be a little bit of overlap, but the other pattern stays out here. So you see, it's not going to be that hard to tell one from the other, particularly out here. What if I get a point way out here? Well, that's pretty certain. That's not part of the two bar, that dot is not part of the two bar experience. So we can tell one from the other. So now what we're going to do is make a little algorithm. We're going to take any point on here, any point on this screen, and we're going to write an algorithm that says what's the probability of that point having occurred as part of a two bar pattern and what's the probability of that point being there because it's part of a diffraction pattern. Now that's easy to do because we have all this data, we have tens of thousands of points we put there just running it ahead of time in the experiment with the timers on so we can tell exactly what point went with which mode. So we know all that. So that's what we'll do. And here's just a little sample way you could do that. You could take a point, any point on this screen and say what is its make a number up five nearest neighbors pick five as an OD number. So let's say two of its nearest neighbors came from a diffraction pattern and three of its nearest neighbors came from a two bar pattern. Okay? So then the probability of a two bar pattern is three over five. The probability of a diffraction pattern is two over five. Simple. So then we move to the next point and the next point. So we're just looking at nearest neighbors and seeing what they are because we know what every one of these came from, right? This is our experiment where we have the time, we have the clocks on, we know everything in R one and exactly what the point was with it. So every point here we know where it came from. Even the ones that are kind of straggly in between these things, we know where they came from. So we're going to make this algorithm and then what we're going to do is we're going to squeeze the larger consciousness system until it gives up some of its secrets. Okay? So hopefully I've managed to put the LCS between the rock and the hard place, thus forcing it to show us some of the VR tricks it's capable of. There are other ways it could work around this predicament, but this is by far the most efficient way that it could do it. I expect it'll do it the most efficient way. So now what we're going to do is we've got this microprocessor. Now it's got this algorithm based on tens of thousands of points that we've run already where we know what the data is. And now we're going to run this thing. And every time a spot lands here, we're going to compute what's the probability that that spot is a diffraction pattern. And of course, one minus that is the probability that's a two bar pattern. So we're going to know that and we're going to know that prediction based on the data long before this thing ever gets to these detectors that it'll either turn that which way data on or turn that which way data off based on a random number from that event based random number generator. The only way it can do this is if it manipulates that radioactive element and makes it hit the hemisphere that it needs it to hit. That's a major miracle, not a minor miracle, because it's not about tiny little particles you can't see. It's about a chunk of radioactive material that is decaying. We know how that works, and it's all random. It doesn't do it selectively. It doesn't do it because, oh, I need it to hit this side or that side. It's random. So now we're saying that this system will make a prediction. And when it's done, we'll look at our predictions and we'll be pretty good because all these points that are way out here, we're not going to confuse them with the two bar pattern because they're way spread out from the two bar pattern. And in here, if we run this enough and if we really optimize our algorithm, and that will take some trial and error to optimize that algorithm and this little thing about the five nearest neighbors, it could be two nearest neighbors or three or any other kind of thing, you could do it, even numbers. And then you could get those that had, say, the even number, and it was eight. You could have four and four, and those numbers would be 0.5. And there's lots of ways to do this, lots of ways to jiggle those patterns around to minimize them. So some of that will just be trial and error till you get your best possible algorithm, your best possible non interference between the two patterns. And then you'll know what your results are. You can run tests against that and say, okay, let's run a whole bunch more where we know exactly what we're getting and see how good our algorithm is. Oh, we predicted them right to 95%, something like that, 80%. So you'll know that that's how it comes out because you've got the time here on this one and you got the time on this one, and you can test how good is it? What's your probability? All right, now we run the real experiment. We're going to run this thing, and we're going to let this switch be activated by this random number. And what happens is this random number generator right here becomes eye candy because this draw determines what that random number is going to be. Major miracle. Right. But this experiment can be done. And if virtual reality works this way and chooses to do this because it's the most efficient, then we will see that we can predict what that event based random decay looks like. Absolutely, totally impossible. Okay, so that's the first major miracle. Now we've got another one that's similar to that. What we're doing is we're forcing our microprocessor is immediately computing that before the switch decides. So it's calling it ahead of time. It's saying in advance, how will that radioactive source decay before it's decayed? All right, now that's the three that were of the anonymous type. Now we're going to go to those that are unique. So now these detectors are going to produce unique signals. Each one's going to label the signal with what slid goes through. So now we're not going to erase here anymore because that unique data is going into R One. Okay, so that's just simple. When you do that unique data is going into R one. We have the which way data. We're going to end up with the two bar pattern. I mean, decoherence. Excuse me, I'm a physicist. I need to say it. We're going to end up with decoherence. So this two bar pattern is what we expect. One happens first. Then we decide the velocity, which gives us the times when these things are going to hit here and here. And then the last thing we do is we have to find out what slit it goes through. And that slit it goes through determines which one of these single value distributions it gets pulled from. All right, easy. Right now we're going to strengthen this one out, and we're going to do a similar kind of game. So now we got a delayed eraser. Again, this now is unique. We're measuring r three. We're measuring r one. Those are the two things that go out to the people. So those are the things that are really important and the which way data is going to be preserved there. Oh, I didn't tell you in that last one what the results were. I'm sorry, I skipped it. The one where we're going to trick the system into telling us what a decaying atom is going to do before it does it. And that is that whenever the switch is off, you're going to get a diffraction pattern because there's no which way data. Whenever the switch is on, you're going to get a two bar pattern. That's what the result is going to be. That switch is going to randomly flip on and off. And when it does, that's what you're going to get. Now, looking at that experiment from the outside, you don't see anything unusual. You don't see anything out of sync or out of line. All you see is the particles come in. You see your swit or your which way data thing being turned on and off randomly, because it all comes from that random distribution, but it comes from the one that picked the answer. But it's still it's a random draw from a binary distribution. So you're going to see a random on and off switching of your which way data. So from the outside, it all looks perfectly normal. Well, for those of you who do simulations, I don't know how many of you actually write simulations, but you know that there's a big difference between the world you simulate and how you do it. It's sort of like behind the curtain. Behind the curtain, you do all kinds of tricks to simplify your simulation so that the result comes out looking perfect just the way it should. But you've got some shortcuts back here and some other things you've done to make it more efficient. That's just the way simulation is. And ours is a simulation. It shouldn't be any different. So we should expect that behind a curtain, there's going to be some tricks. As long as when it comes out here to this virtual reality, it looks perfect. So even though it's a major miracle, it looks perfect out here. And all the times the switch is off diffraction all the time. The switch is on two bars. All right. So now we have this one. Okay. You know that a two bar pattern occurs only when there's objective which way data. And here we're going to have it, even though it's going to take us a long time to get it. So there's really nothing too amazing about this. The interesting one, I guess, is going to come next. Okay. Now we're going to play a similar trick on Mother Nature that we already did play a trick on the larger kinds of system. This time, though, we are going to not do something to each one of these two paths. We're actually going to drop down here and put a fast switch that just disconnects R One. The reason I like this one, this one and the other one are very similar, but they have these differences. This is cutting off the detector, which is a different thing than adding information to the two paths. It's a different logic, so it's testing a little different logic. And secondly, we're removing which way data rather than adding it because we're going to let that which way data get right down to the point that it's almost into R One, and we're going to disconnect R One and not have any data at all. So we're not going to have any which way data. The thing that I like about it most, though, is fast switches are cheap and easy. That which way data injector is going to take a little bit of finagling on the part of the experimenters. This is a fast switch. You can get a fast switch cheaply and easily. So it makes this experiment actually more doable than the other one, which is why I didn't want to leave it out because I'll let. The experimenters decide which they find the easier to do or they can do them both. But it's a very similar sort of thing. So now we're going to turn R One off right there after we've written this data. So it's a similar kind of thing. So R One sitting there, it's collecting which way data because these are unique detectors now and the particle is going to come through here. It's get to a screen, the system is going to have to say, what am I going to put on that screen? It goes up here and it decides the answer. It's going to put a diffraction pattern or a single pattern. It writes it to the screen. Then it has to manipulate this switch and make the switch be on or off to fit what it picked up here. And it's the same thing. So if it's a diffraction pattern, that switch has to be off and it's done. If it's a single value, then it has to make another draw so we can say which lid it went through and then write to R One at the appropriate time. And again, it's going to look like that fast switch is just going on and off randomly. If you look at the pattern of it going on and off, it would be perfectly random because this is perfectly random and that's what's driving it. So to the outside world, everything looks normal. And we're going to trick the system in the same way. We're going to put a microprocessor on it. And that microprocessor is going to be programmed with something that will tell one pattern from the other. The patterns are going to be deconflicted and that microprocessor will say when this signal is only in here someplace. When this signal gets all the way down to that switch, that switch is going to be off or on and it should be right like 80, 90% of the time because we ought to be able to deconflict those patterns to that degree. It won't necessarily be perfect all the time because some of those points it's really hard to tell which pattern they're a part of. But for the most part it's going to be a whole lot better than random. So that's the next miracle. That's very similar to the first one, just different way, different thing, a little cheaper to build and it tests a couple of things besides what the last one did. There was an experiment that we're going to get to next that this kind of actually depends on. So I guess maybe we'll do that next. Here's one that is this is kind of implicit in that other experiment, but I pulled it out here separately. And that is we have the two slits. We have the detectors on detecting and we just turn off the recorders. Nothing's being recorded. If nothing's being recorded, nothing is going to an observer. If nothing's going to observer, nothing changes. So if nothing is being recorded, then, yes, it's computing which way data, but it's not saving it. So there's no observer can observe it. So what we're going to get is a diffraction pattern. I did this though, I don't know, years ago in some lectures I talk about this and gave this example. I've probably gotten more flak for that than I have for anything else because I kept getting are you sure that's been done? I can't find that anywhere in the literature. Point me to where that's been done. I can't find it anywhere in the literature either. That's why I'm putting it up here. I can't believe it hasn't been done. It is too trivial and too obvious not to do because there's this big thing in the physics community and elsewhere that it's the detectors here interfering with the slit. And it's those detectors that are causing this thing to collapse to a particle and so on. Well, this is just a very simple experiment that puts that to rest. Right. Let the detectors detect. Just turn off the recorders and you'll immediately know whether the detectors have anything to do with the collapse of the wave function. It's not really what's going on, but it's a neat metaphor. So this experiment is, just for the record, to set this straight. And if this doesn't work, then a lot of the rest of my stuff probably not going to work either because this is a fundamental virtual reality. Without the observer, there is no change. There isn't anything. So again, here we have the you know, we have the one and the two that we need. And then we don't have any data here because this is off. There's no binary which way. We don't care which slid it goes through. It doesn't make any difference because we're not going to record any of it. All we have is a draw here. And because there is no which way data, we draw in a diffraction pattern. So this whole thing is done with a single draw out of this distribution. Plus, of course, that draw out of the random to make the Y. So that's how I predict that one will work. So it really tests several VR concepts? It tests the concept. Is our reality virtual? Is it computed? If it is computed, is it computed parsimoniously? Our virtual reality is generated by an intelligent probability based simulation, not a deterministic. One has to do with an observer. If you don't have an observer, then it doesn't mean anything because it's no change. Our world is created in the mind of the observer. No observer, no world to create. So this all virtual reality concepts and all of them kind of center on this experiment. They kind of stand or fall based on how that experiment comes out. I can't help but think that back in the 1920s this was done, but stuff done in 1920 is hard to come by. It's not really available anymore. So we want to redo that just to kind of settle that argument. All right. And that was experiment two. Now we're going to get the three coming up next. And three is a whole set of experiments that I'd like to see done. There are no miracles in here. It's just trying to understand better how consciousness and humans interact. And unfortunately, even though physicists know that the observer is important in the Double Sluid experiment, they really don't like that. It's not a happy thought. So they have ignored it. And nobody's really done a lot of experiments to say just how do you define that observer? What does that observer have to do? How do you define which way data? If he just glances at it as it goes across the screen and disappears, does that count or does it have to be recorded? So if he glances at it and writes it down on a notepad, is it recorded? Is that all it counts? Or does it have to be recorded on a device? There's all these things about the experiments and the people that, as far as I can tell, have never been studied. So it's a completely wide open area of study of the man experiment interface, and it'll define a lot of things about what really defines which way data and what really defines an observer. What if it's deductive logic versus inductive logic? If you can pin down the result to one in a billion, but you're still not entirely sure, there's a little bit of uncertainty, does that count as objective data? Well, you can do that with inductive reasoning or not. You see, there's lots and lots of questions here, but there's absolutely no data to solve them. And in science, the only way you solve those is with an experiment. So I'm asking for these experiments to be done because it'll just make more sense. And the way I've done these is I've replaced R one and R two with people. So now the people are the recorders and the observers, and we're going to run I've got a whole series of these. You can see there's lots of them from three A one to three C, a one, A two, A three, and so on. There's lots of them and it's just different configurations to see how they work, what defines objective. So I have these guys standing here and at first, the first experiment, and I've got all these experiments written down some place on ten point type that nobody will be able to read. But I put them again all on one sheet because it's really convenient to go back and forth between them. I have one where, of course, the men have oscilloscopes. That's what that little thing is. That's an oscilloscope there they're looking at. And when they get a pulse, they get a Blip on their oscilloscope. Not recorded. Oscilloscope doesn't have memory. It's got persistence. So it maybe lasts a half a second or so they see it, they know. But yet somebody has seen a human being, a consciousness has seen that a pulse did go through slit one or a different guy through slit two. Now they can just stand there and be silent and watch. Well, is that enough to define which way data? They can take notes. Is that enough? Well, they can call them out so when they see one they can say pulse in one, so they can hear each other. They can count the pulses and know that between the two of them the pulses they count are exactly the same as the number of dots on a screen which verifies they saw every single one go through one of the slits or the other. Is that enough to define which way data see, none of this falls in the parameters of what we've done because we either record which way data precisely on a machine or a recorder, or we don't. Well, this is now looking at all the gray area in between and it's just never been done before. And I have let's see, they record time, not just the pulse, but they write down the time. I have one where a third guy comes out and looks at the screen and they each call out things and when the thing hits the screen, the guy on the screen, a light goes on. So now we have correlation among all three of them. All three of them know that a pulse just went through one slit or the other and correlates with a dot on the screen. Is that enough? Nothing's written down, nothing's recorded, it's all in their heads. That's got a subjective component to it, subjective data, but it's subjective storage. Does that work? I have no idea. Well, I do have some ideas, but I really don't have any final answers on how all that works. But I just like to know because that will tell me a lot about how the system works and how it interfaces with people. And the research has never been done, so it's a simple thing to do. And of course I have one where they do everything, where they have a little recorder and they say slit one just got a pulse and the time is that sort of thing. So now you have the time. The fact that it happened at a pulse and it's on a recorder, well, that should be just the same as having R one and R two turned on. That certainly will produce a two bar pattern. So I go from one extreme to the other to where it just barely qualifies, to where it's completely qualified. And to me these would be interesting and should have been done 100 years ago, but have it. All right, this is a little bit about experiment three. Experiment three suggests eight experiments and several additional sub experiments depending on how the initial experiments turn out. These experiments are only initial suggestions for the exploration of consciousness connection with quantum experiments. Their result will point to new experiments. And this just lists some of the issues that are in play here. Is human memory subjective with some uncertainty? Is that sufficient to be the data recorder? Or must it be entirely objective without uncertainty? Does an individual's subjective experience of an objective event count as a measurement? Or must the event be recorded on objective media? Equally variable to all? Does it matter who or what the observer is? Would a professional physicist or a trained chimp do equally as well? Is the objectivity of which way data defined by sending data to any PMR avatar, which would include the chimp? Or only requirement that really matters, or is the only requirement that really matters is avoiding a noticeable PMR conflict? Oh, we can have PMR conflicts as long as nobody notices or cares, in which case the chimp wouldn't notice or care, you see? So lots of soft gray areas that need to be explored. Okay? And then how does the persistence or volatility of the which way data matter? You see, that pulse only exists for that guy to see, and then it's gone. There's no record of it. It's gone. It's volatile data. Well, let's say we have a scope that has longer persistence. Now it persists for a second. Now 10 seconds. Now, let's say the scope persists for 10 hours. What about ten days? What about a week? Well, if it persists for that long, it's starting to look like a recorder, doesn't it? It's a recorder that the media goes bad after a while. It's like recording on old cassette tapes. Ten years later, they're all gone to dust. So every measurement actually is perishable. Then the question is, well, how perishable can it be? What if he only looks at it for 100th of a second? What if it flashes on the screen so fast that he doesn't really see it, but he only gets it in his subconscious, you see? I mean, you can go on and on and on. You can make 100 experiments out of this that looks at all these gray areas that nobody knows anything about. So what I've done on the next slide, which I'm sure you'll want to read, I come down here and I do some of the basic experiments, and then I do a split, and I say, if this turns out to be a diffraction pattern, then do this, this, and this. If it turns out to be two bars, then do that, that, and that, because it's going to suggest its own next experiments. So this is not meant to be complete. This is meant to be a teaser, a start to do some solid research in this area of the observer and experiment interface, something that's just been ignored. Okay, there's more. More all kind of discussions. If it goes this way, why it might go that way, and if it goes the other way, what may be the issues be and all of this stuff that nobody here probably really wants to hear now. That's why it's in the small print. You get it later if you really want. All right. Yeah. This is one of my favorites. This is one that I talked about five, six, seven years ago. I've had these experiments for a long time. I just didn't think the time was ready yet. There's no point putting them out until virtual reality is a big enough deal in physics departments that they'll actually do the experiments. If I put these out a decade ago, everybody would have just laughed and forgotten about it. So now, though, virtual reality is a going thing. Every physics department has physicists that are serious about virtual reality. It's the big thing in physics now because it's the only thing that actually answers the questions and gives the answers to why experiments turn out the way they do. So this is what I called in my earlier lectures, the envelope experiment. The envelope experiment was you take this standard setup where you turn all these things on, all the detectors on the recorders are on. You're going to get a two bar pattern. Everybody knows that. It happens all the time, right? When you do this configuration. Well, then you run this a whole bunch of times. So in my talks, I say, okay, run it 102 times. So you got 102 of these that you've run, and each one you run, you take the data here from R one, and you take the data here that's on the screen. You don't look at it. It's just data in the computer. And you take it off and you put in an envelope and you label that envelope experiment number one, which got two other envelopes in it, one for the detector data, one for the screen data. And you do that for all 102 of them. And then you take the first one that you did. Number one, you look at it. You look at the screen data, you look at the detector data, and sure enough, you are going to get two bars, because that's what this experiment does. You look at the last one you did, and unless your experiment messed up somewhere in between, you're going to get two bars, because that's what the experiment does. But now you've got a whole bunch more that have never been looked at. Now what you do then is you wait for a year or a decade or a day, it doesn't matter. And you come back later and you get these envelopes out, and you shuffle there's 100 of them left, right? We had 102. You shuffle them all up, randomize them. You separate them in two piles, 50 and 50. This 50, you take the detector data and destroy it. This 50, you leave it alone. And then the result I predict is that all the ones that had their detector data destroyed a decade later will all have diffraction patterns. All the ones that have their detector intact will not. Now, this has a couple of major concepts in it. One is this is a delayed eraser experiment, except eraser is taking place long time. It's a really long delay. Okay, so we're testing the fact that I told you about in one of the other slides, that if the logic of the system hasn't closed out, it's not fixed to where it can still change. The system hedges its bets and waits to the logic can't change anymore. And when the logic is fixed, settled, then that's when it computes the answer. But until that logic is settled, it doesn't compute the answer. Now, if that took a decade before it was settled because you had all this data and you didn't know what was in it, well, you have a pretty good idea because this experiment just runs one kind of thing. So your inductive reasoning would say they're all going to be two bars. That's what the experiment does. And that's probably, as inductive reasoning goes, a very high probability and a very reasonable thing to say. But on the other hand, it's not objective. It's a good guess, but it still has some uncertainty to it. So does the which way data have to be objective? Or does subjective work? Does it only has to almost be objective? And if so, how close to almost do you have to get before it changes? See, I think that the system doesn't want to get in that kind of a crack between how much almost do you have to get before it changes. We don't want that. We want it to be something that is always the same. And I would say that's going to be objective. So my thought is that this experiment will work that way. And what I've done here is I've redone this in terms of thumb drives. Okay? So here's what happens. First, set up, test and use standard double slit experiment recording which way information on R One, R two and screen data on R three, R one, R two and R three are removable flash drives. Repeat this experiment ten times so there'll be ten subexperiments each time. Using a new set of flash drives. Label all drives R one, R two, R three and keep R One, R two and R three for each subexperiment together, each marked with its subexperiment number from one to ten. Immediately secure the flash drives after each subexperiment. No. 1 may look at, write, copy or duplicate any flash drive data. Make certain that the data handling system cannot possibly contain any minute residue of the data that's on the flash drives. So that all has to be erased completely, completely, physically destroy, and I say crush and or melt, either to liquid or to smoke. It's amazing what the forensic computer forensics can dig out of people trying to get rid of data they don't want anybody to see. I mean, those guys are good at pulling data out, almost nothing out of the ashes. So really destroy these things and then randomly choose five let's see what I say. Into smoke in R two, four, five randomly chosen sub experiments. We're going to take five sub experiments out of the ten, and we're going to destroy the detector data for them. All right, then. Next is look at R three. That's a screen result data for all ten experiments. Prediction the result screen data for the subexperiments with destroyed detector data will show diffraction patterns and that of all others. Those with preserved detector data will show two bars. That's almost a major miracle, but not quite. That would be unexpected. You see, we've only done these delayed erasers in terms of nanoseconds. The delay is very small. The particles are very small. We've not done this in a macro sense with information on flash drives. Instead, we've always done with tiny little things and tiny little times. And that just leaves a lot of uncertainty in people's minds. And it's actually worked. There's been some recent experiments where they've done something like this, and it works exactly the way I said. But it's a very complex experiment, so it leaves a lot of uncertainty. This is not complex at all. This is a very big experiment. And how long they wait before they do that destruction and pull those flash drives out doesn't matter. And I'm sure as far as physics is concerned, if they wait five minutes, that's enough. Five minutes is good as five years, right? Time doesn't matter there. Okay? So that's what we're doing. We are to let's see objective conditions that exist at the time and IOC looks at the results. So we're going to test that assumption that it's the individual unit of consciousness looking at. The result is when that last piece of the logic is done. Otherwise, that's optional, that's still open, could still change the experiment. So it's not until that's done that the system decides what it's going to do, what its final word is, even if that's a decade later. And it also explores the experimental logic that remains open to change. That if it remains open to change, then the system remains open to change until an IUOC requests the data. See, because until somebody requests that data, which means somebody looks at it, it's neither here nor there. It doesn't exist or not exist in this physical reality. It only gets into this physical reality when the observer looks at it, asks for the measurement, and gets the data. And that time the system has to decide what to put in their data stream, not before. So ten years later, when somebody pulls that stuff out, destroys a random half of it, and then looks at all the screen data, when they look at that screen data. Now the system has to put something in their data stream to let them know what they see. Didn't have to do any of that before. So this is the first time that the result actually gets into this physical reality. And when it does, I'm thinking that it will do so with the logic that's present now, not with the logic that was present in the past. That that logic goes with the writing to the data stream. Okay, so we're not talking about micro eraser. We're talking about a macro eraser. We're not talking about little detaily times. We're talking about as long a time as you want can which way data be rendered objective by inductive logic? Inductive logic would say you're just going to get two bar patterns, because that's what that experiment does. But there's uncertainty in that because nobody's seen the data. It's not in this reality yet. That's a guess. Good guess, but still a guess. Interesting experiment, I think. Like to see the outcome. All right, we got more of that information pertinent to that experiment, so I want to skip that one and then this next one. What I've done here is if the experiment four unexpectedly fails to show Macularasure effect, then do these experiments. And what these experiments are going to do is try to determine just how much probability in that inductive reasoning do you need to have before it changes non which way data to which way data? I mean, how close do you have to be to one in that probability before it's called objective? And these then would tend to tease out some of that data. So I have some experiments on either side of that issue, but I think it will happen the way I said it first. All right, now we're on the next one, and we can take our break now.

